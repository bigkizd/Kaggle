{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-100.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toandaominh1997/Kaggle/blob/master/CIFAR/CIFAR_100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cACzhuHI8w5K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EP0fxHFk-B5P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolution Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "LURWQrfr-C1p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import urllib\n",
        "import sys\n",
        "import tarfile\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p1CbINZA7Lm5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3560c7b-372b-49c9-b2d2-821b96c9e13e"
      },
      "cell_type": "code",
      "source": [
        "URL = \"https://www.cs.toronto.edu/~kriz/\"\n",
        "filename = \"cifar-100-python.tar.gz\";\n",
        "path = os.path.join(\"datasets\", \"cifar\")\n",
        "def download_process(count, block_size, total_size):\n",
        "  percent = count*block_size*100/total_size\n",
        "  sys.stdout.write(\"\\rDownloading {}%\".format(percent))\n",
        "  sys.stdout.flush()\n",
        "  \n",
        "def fetch_pretrain(filename):\n",
        "  if(os.path.exists(URL+filename)):\n",
        "    return\n",
        "  os.makedirs(path, exist_ok=True)\n",
        "  gz_path = os.path.join(path, filename)\n",
        "  urllib.request.urlretrieve(URL+filename, gz_path, reporthook=download_process)\n",
        "  ex_path = tarfile.open(gz_path)\n",
        "  ex_path.extractall(path=path)\n",
        "  \n",
        "\n",
        "fetch_pretrain(\"cifar-100-python.tar.gz\")\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 100.00456504994096%"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sAk0lw2e8wb2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "dict_train = unpickle(os.path.join(path, \"cifar-100-python/train\"))\n",
        "dict_test = unpickle(os.path.join(path, \"cifar-100-python/test\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_BVsIuq2C-tG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename_train = dict_train[b'filenames']\n",
        "fine_label_train = dict_train[b'fine_labels']\n",
        "data_train = dict_train[b'data']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "egWVkO9EFCGy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IlT5AMQvDnME",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename_test = dict_test[b'filenames']\n",
        "fine_label_test = dict_test[b'fine_labels']\n",
        "data_test = dict_test[b'data']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X0-VINnNEzlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67a89bcd-2690-44db-8da6-d62e85d71e78"
      },
      "cell_type": "code",
      "source": [
        "data_train[0].shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3072,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "NiRWz-GrFfKQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a02fb2c8-d3fd-45c8-9c9e-877e9322d611"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(data_train.shape)\n",
        "print(data_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 3072)\n",
            "(10000, 3072)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BxTiyw2EFlKG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cea1da01-dcb3-49cf-88d2-c2dd3169f7d8"
      },
      "cell_type": "code",
      "source": [
        "X_train = np.concatenate((data_train, data_test))\n",
        "y_train = np.concatenate((fine_label_train, fine_label_test))\n",
        "X_train =np.asarray(data_train)\n",
        "y_train=np.asarray(fine_label_train)\n",
        "\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(type(X_train))\n",
        "print(type(y_train))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 3072)\n",
            "(50000,)\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mG-iw9tgkJm5",
        "colab_type": "code",
        "outputId": "bddb5d69-030f-4bde-ca59-d2ecf6b60345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "URL = \"https://www.cs.toronto.edu/~kriz/\"\n",
        "filename = \"cifar-100-python.tar.gz\";\n",
        "path = os.path.join(\"datasets\", \"cifar\")\n",
        "def download_process(count, block_size, total_size):\n",
        "  percent = count*block_size*100/total_size\n",
        "  sys.stdout.write(\"\\rDownloading {}%\".format(percent))\n",
        "  sys.stdout.flush()\n",
        "  \n",
        "def fetch_pretrain(filename):\n",
        "  if(os.path.exists(URL+filename)):\n",
        "    return\n",
        "  os.makedirs(path, exist_ok=True)\n",
        "  gz_path = os.path.join(path, filename)\n",
        "  urllib.request.urlretrieve(URL+filename, gz_path, reporthook=download_process)\n",
        "  ex_path = tarfile.open(gz_path)\n",
        "  ex_path.extractall(path=path)\n",
        "  \n",
        "\n",
        "fetch_pretrain(\"cifar-10-python.tar.gz\")\n",
        "data = []\n",
        "label = []\n",
        "for i in range(1, 6):\n",
        "  batch = unpickle(os.path.join(\"datasets/cifar/cifar-10-batches-py\", \"data_batch_\"+str(i)))\n",
        "  \n",
        "  data.append(batch[b'data'])\n",
        "  label.append(batch[b'labels'])\n",
        "\n",
        "for i in range(1, len(data)):\n",
        "  data[i] = np.concatenate((data[i-1] , data[i]), axis=0)\n",
        "  label[i]= np.concatenate((label[i-1], label[i]), axis=0)\n",
        "X_train = data[len(data)-1]\n",
        "y_train = label[len(label)-1]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(type(X_train))\n",
        "print(type(y_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 100.0011876967218%(50000, 3072)\n",
            "(50000,)\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xiXt9SXvHxip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IdivestJGZ8R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "num_steps = 2000\n",
        "batch_size = 128\n",
        "\n",
        "num_input = 3072\n",
        "num_classes=100\n",
        "dropout=0.25\n",
        "\n",
        "def conv_net(X, n_classes, dropout, reuse, is_training):\n",
        "  with tf.variable_scope('ConvNet', reuse=reuse):\n",
        "    x = tf.reshape(X, shape=[-1, 32, 32, 3])\n",
        "    conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
        "    conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
        "    conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
        "    conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
        "    fc1 = tf.contrib.layers.flatten(conv2)\n",
        "    fc1 =tf.layers.dense(fc1, 1024)\n",
        "    fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
        "    out = tf.layers.dense(fc1, n_classes)\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ywsVtzX6G180",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_fn(features, labels, mode):\n",
        "  logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
        "  logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
        "  \n",
        "  pred_classes = tf.argmax(logits_test, axis=1)\n",
        "  pred_probas = tf.nn.softmax(logits_test)\n",
        "  if(mode==tf.estimator.ModeKeys.PREDICT):\n",
        "    return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
        "  loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
        "  train_op = optimizer.minimize(loss_op, global_step = tf.train.get_global_step())\n",
        "  \n",
        "  acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
        "  estim_specs = tf.estimator.EstimatorSpec(mode=mode, predictions=pred_classes, loss=loss_op, train_op=train_op, eval_metric_ops = {'accuracy':acc_op})\n",
        "  return estim_specs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AyRXO95lJ_pk",
        "colab_type": "code",
        "outputId": "f45f939b-19a7-4395-a736-1f162ba4ba4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "model = tf.estimator.Estimator(model_fn)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp9w5z13l7\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp9w5z13l7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f12e00ed208>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JoIqUTjPKD4v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = np.asarray(X_train, dtype=np.float32)\n",
        "y_train = np.asarray(y_train, dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JkKw7VPx7rV-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "dc8e7d5e-c2a5-4b27-d18b-fa81ec9517d9"
      },
      "cell_type": "code",
      "source": [
        "# Define the input function for training\n",
        "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x=X_train, y=y_train,\n",
        "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
        "# Train the Model\n",
        "model.train(input_fn, steps=num_steps)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp9w5z13l7/model.ckpt.\n",
            "INFO:tensorflow:loss = 152.50842, step = 0\n",
            "INFO:tensorflow:global_step/sec: 80.6285\n",
            "INFO:tensorflow:loss = 4.607315, step = 100 (1.244 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.1448\n",
            "INFO:tensorflow:loss = 4.607752, step = 200 (1.077 sec)\n",
            "INFO:tensorflow:global_step/sec: 92.742\n",
            "INFO:tensorflow:loss = 4.6260257, step = 300 (1.078 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.2559\n",
            "INFO:tensorflow:loss = 4.548916, step = 400 (1.062 sec)\n",
            "INFO:tensorflow:global_step/sec: 92.8245\n",
            "INFO:tensorflow:loss = 4.5421276, step = 500 (1.074 sec)\n",
            "INFO:tensorflow:global_step/sec: 96.0833\n",
            "INFO:tensorflow:loss = 4.5496006, step = 600 (1.043 sec)\n",
            "INFO:tensorflow:global_step/sec: 95.3837\n",
            "INFO:tensorflow:loss = 4.556616, step = 700 (1.045 sec)\n",
            "INFO:tensorflow:global_step/sec: 96.301\n",
            "INFO:tensorflow:loss = 4.5955544, step = 800 (1.039 sec)\n",
            "INFO:tensorflow:global_step/sec: 97.8213\n",
            "INFO:tensorflow:loss = 4.4615664, step = 900 (1.027 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.5983\n",
            "INFO:tensorflow:loss = 4.378831, step = 1000 (1.065 sec)\n",
            "INFO:tensorflow:global_step/sec: 97.1777\n",
            "INFO:tensorflow:loss = 4.4749427, step = 1100 (1.032 sec)\n",
            "INFO:tensorflow:global_step/sec: 96.0595\n",
            "INFO:tensorflow:loss = 4.1353893, step = 1200 (1.041 sec)\n",
            "INFO:tensorflow:global_step/sec: 96.9149\n",
            "INFO:tensorflow:loss = 4.2195783, step = 1300 (1.028 sec)\n",
            "INFO:tensorflow:global_step/sec: 97.2358\n",
            "INFO:tensorflow:loss = 4.256913, step = 1400 (1.029 sec)\n",
            "INFO:tensorflow:global_step/sec: 96.4518\n",
            "INFO:tensorflow:loss = 4.0878973, step = 1500 (1.039 sec)\n",
            "INFO:tensorflow:global_step/sec: 95.3816\n",
            "INFO:tensorflow:loss = 4.1918497, step = 1600 (1.049 sec)\n",
            "INFO:tensorflow:global_step/sec: 95.1505\n",
            "INFO:tensorflow:loss = 4.0293965, step = 1700 (1.048 sec)\n",
            "INFO:tensorflow:global_step/sec: 95.2664\n",
            "INFO:tensorflow:loss = 4.1441426, step = 1800 (1.052 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.3136\n",
            "INFO:tensorflow:loss = 4.0404925, step = 1900 (1.060 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2000 into /tmp/tmp9w5z13l7/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 3.9566069.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.estimator.estimator.Estimator at 0x7f12cbcedba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "zpOvimwFKggD",
        "colab_type": "code",
        "outputId": "8dde8614-1b9c-4b76-b40d-cc89c849e590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Evaluate the Model\n",
        "# Define the input function for evaluating\n",
        "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "    x=X_train, y=y_train,\n",
        "    batch_size=batch_size, shuffle=False)\n",
        "# Use the Estimator 'evaluate' method\n",
        "model.evaluate(input_fn)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-11-06-12:20:19\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmp9w5z13l7/model.ckpt-2000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2018-11-06-12:20:22\n",
            "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.1007, global_step = 2000, loss = 4.0331964\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /tmp/tmp9w5z13l7/model.ckpt-2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.1007, 'global_step': 2000, 'loss': 4.0331964}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "nQMhBbOl-O5T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Keras Resnet50"
      ]
    },
    {
      "metadata": {
        "id": "6TYGgM-gUPwm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "\n",
        "nbr_of_clases = 10\n",
        "validation_percentage = 0.2\n",
        "resnet_path = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "def prepare_data_for_resnet50(data_to_transform):\n",
        "    data = data_to_transform.copy().values\n",
        "    data = data.reshape(-1, 28, 28) / 255\n",
        "    data = X_rgb = np.stack([data, data, data], axis=-1)\n",
        "    return data\n",
        "y = np.zeros((y_train.shape[0], 10))\n",
        "\n",
        "for i in range(0, y.shape[0]):\n",
        "   y[i, int(y_train[0])-1]=1\n",
        "  \n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y, test_size=validation_percentage)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wr7rtjMMYjDR",
        "colab_type": "code",
        "outputId": "79ab83b0-a6c5-4022-dfce-a60f1593a1c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(ResNet50(include_top=False, pooling='avg'))\n",
        "\n",
        "model.add(Dropout(0.50))\n",
        "model.add(Dense(nbr_of_clases, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94658560/94653016 [==============================] - 9s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_SPlGEKhvVH_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_model(model, epochs=1, train_test_split=0.0):\n",
        "    model.fit(X_rgb, y, epochs=epochs, validation_split=train_test_split)\n",
        "    \n",
        "def get_fitted_data_generator(data):\n",
        "    data_generator = ImageDataGenerator(rotation_range=10, width_shift_range=0.1,\n",
        "                                   height_shift_range=0.1, zoom_range=0.1)\n",
        "    data_generator.fit(data)\n",
        "    return data_generator\n",
        "    \n",
        "def fit_model_generator(model, X_train, y_train, epochs=1, batch=32, validation_data=False, X_val=None, y_val=None):\n",
        "    image_nbr = np.size(X_train, 0)\n",
        "    training_data_generator = get_fitted_data_generator(X_train)\n",
        "    \n",
        "    if validation_data:\n",
        "        return model.fit_generator(training_data_generator.flow(X_train, y_train, batch_size=batch), steps_per_epoch=(image_nbr//batch),\n",
        "                        epochs=epochs, validation_data=(X_val, y_val), verbose=1)\n",
        "    else:\n",
        "        return model.fit_generator(training_data_generator.flow(X_train, y_train, batch_size=batch), steps_per_epoch=(image_nbr//batch),\n",
        "                        epochs=epochs, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rIF75ZDQfo9s",
        "colab_type": "code",
        "outputId": "5a4fd748-2e75-499d-865b-c5066b1e8171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model_history = fit_model_generator(model, X_train, y_train, epochs=1, validation_data=True, X_val=X_val, y_val=y_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "522/522 [==============================] - 162s 311ms/step - loss: 0.0324 - acc: 0.9928 - val_loss: 6.4699e-06 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TZsYNUI5xbjb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_predictions(model, data):\n",
        "    return np.array([np.argmax(prediction) for prediction in model.predict(data)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EZoNvA5BxcYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predicted = get_predictions(model, X_val)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kr5lHDnlx-0J",
        "colab_type": "code",
        "outputId": "cac255bd-f354-4449-c8a2-423910247f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "yval=[]\n",
        "for i in range(0, y_val.shape[0]):\n",
        "  pos=0\n",
        "  for j in range(0, 9):\n",
        "    if(y_val[i, j]==1):\n",
        "      pos=j;\n",
        "      break\n",
        "  yval.append(pos)\n",
        "print(len(yval))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eBzdv7VShvG0",
        "colab_type": "code",
        "outputId": "58494b2f-5ad6-4ab8-ff40-7d051fa0662e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Validataion : \", accuracy_score(predicted, yval))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validataion :  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_MUoXNvfxyIa",
        "colab_type": "code",
        "outputId": "59e052d6-e3be-409f-cd22-800ae34021e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/9b/ac57e15fbb239c6793c8d0b7dfd1a4c4a025eaa9f791b5388a7afb515aed/kaggle-1.5.0.tar.gz (53kB)\n",
            "\r\u001b[K    19% |██████▏                         | 10kB 19.2MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 20kB 1.7MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▌             | 30kB 2.5MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▋       | 40kB 1.7MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 51kB 2.1MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.10.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Collecting python-slugify (from kaggle)\n",
            "  Downloading https://files.pythonhosted.org/packages/00/ad/c778a6df614b6217c30fe80045b365bfa08b5dd3cb02e8b37a6d25126781/python-slugify-1.2.6.tar.gz\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Collecting Unidecode>=0.04.16 (from python-slugify->kaggle)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 7.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle, python-slugify\n",
            "  Running setup.py bdist_wheel for kaggle ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8b/21/3b/a0076243c6ae12a6215b2da515fe06b539aee7217b406e510e\n",
            "  Running setup.py bdist_wheel for python-slugify ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e3/65/da/2045deea3098ed7471eca0e2460cfbd3fdfe8c1d6fa6fcac92\n",
            "Successfully built kaggle python-slugify\n",
            "Installing collected packages: Unidecode, python-slugify, kaggle\n",
            "Successfully installed Unidecode-1.0.22 kaggle-1.5.0 python-slugify-1.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0FSNCAAc0RwB",
        "colab_type": "code",
        "outputId": "89acb858-f0cf-4dbb-9f6c-9fa6b8d7d904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "path = \n",
        "path_file = os.path.join(\"datasets\", \"CIFAR\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 120.13491714327614%"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y0utfLVo0-b8",
        "colab_type": "code",
        "outputId": "44244833-f651-4695-a6a4-5421e54bcf95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pyunpack\n",
        "!pip install patool\n",
        "from pyunpack import Archive\n",
        "Archive(os.path.join(path, \"test.7z\")).extractall(path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyunpack in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: easyprocess in /usr/local/lib/python3.6/dist-packages (from pyunpack) (0.2.3)\n",
            "Requirement already satisfied: patool in /usr/local/lib/python3.6/dist-packages (1.12)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "PatoolError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPatoolError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-907c4419700b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install patool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyunpack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArchive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mArchive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test.7z\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyunpack/__init__.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, directory, auto_create_dir, patool_path)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall_patool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatool_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'zipfile'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyunpack/__init__.py\u001b[0m in \u001b[0;36mextractall_patool\u001b[0;34m(self, directory, patool_path)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mPatoolError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'patool timeout\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPatoolError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'patool can not unpack\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextractall_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPatoolError\u001b[0m: patool can not unpack\nERROR: /content/datasets/CIFAR/test.7z\n/content/datasets/CIFAR/test.7z\nOpen ERROR: Can not open the file as [7z] archive\n\n\nERRORS:\nIs not archive\npatool error: error extracting /content/datasets/CIFAR/test.7z: Command `['/usr/bin/7z', 'x', '-y', '-o/content/datasets/CIFAR', '--', '/content/datasets/CIFAR/test.7z']' returned non-zero exit status 2"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4UsqJbWa2Nz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "31a4f238-e3da-4cff-dc3f-57e3fdf1955d"
      },
      "cell_type": "code",
      "source": [
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-614640d4fa8f>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KT6H4YDS80_i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y=mnist.train.labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PJEUtQeu84em",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce09c1c0-f69b-4f96-9bda-0070693fdc15"
      },
      "cell_type": "code",
      "source": [
        "print(y[:10])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7 3 4 6 1 8 1 0 9 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BtayBvgC85uD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}